{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47950548",
   "metadata": {},
   "source": [
    "# Introduction to the NLTK library for Python\n",
    "    NLTK (Nautral Language Toolkit) is a leading platform for building python program for work with human language data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5bdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e4dc8",
   "metadata": {},
   "source": [
    "# 1. Sentence Tokenization \n",
    "Is also called **sentence segmentation** is the problem of dividing a string of wirtten language into its component sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2b9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a398d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b76d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5504a",
   "metadata": {},
   "source": [
    "# 2. Word Tokenization\n",
    "Is also called as **Word Segmentation** is the problem of dividing string of written language into its components words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36dc300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38eeea2",
   "metadata": {},
   "source": [
    "# 3. Text Lemmatization and Stemming\n",
    "For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. \n",
    "The goal of both Stemming and Lemmatization is to **reduce inflectional forms** and sometimes **derivationally related forms** of a word to a common base form.\n",
    "\n",
    "Eg: dog, dogs, dog's, dogs' => dog \n",
    "    the boy's dogs are different sizes => the boy dog be differ size.\n",
    "    \n",
    "**Stemming** refers to a crude heuristic process that chops off the ends of words in the  hope of achieving this goal correctly most of the time.\n",
    "Eg: The word \"better\" has \"good\" as its lemma. This link is missed bt stemming, as it requires a dictionary look-up.\n",
    "\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words.\n",
    "Eg: The word \"meeting\" can be either the base form of a noun or a form of verb (\"to meet\") depending on the context. Lemmatization attempts to select the correct lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05ad5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1946aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Stemmer: play\n",
      "Lemmatizer: play\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare(stemmer, lemmatizer, word, pos):\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "compare(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "compare(stemmer, lemmatizer, word = \"playing\", pos = wordnet.VERB)\n",
    "compare(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e083fa86",
   "metadata": {},
   "source": [
    "# 4. Stop Words\n",
    "Stop words are words which are **filterred out** before or after processing of text. when applying machine learning to text, these words can add a lot of **noise**.\n",
    "\n",
    "Stop words usually refer to the most common words such as \"and\", \"the\", \"a\" in a language, but there is **no single universal list** of stopwords. \n",
    "\n",
    "the NLTK tool has a predefined list of stopwords that refers to the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e99d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457cca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "362de230",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f2d66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc2df9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ab5a3",
   "metadata": {},
   "source": [
    "We convert List to Set. Set is an abstract data type that can store unique values, without any particular order. The **Search operation is a set is much faster** than the **Search operaion in a list**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaa2fd",
   "metadata": {},
   "source": [
    "# 5. Regex\n",
    "A regular expression, regex is a **sequence of characters** that define a **search patter** and is a powerful tool for **pattern-matching**.\n",
    "We can use regex to apply additional filtering to our text. For example we can remove all the non-word characters. In many cases, we dont need the punctuation mark and its easy to remove them with regex.\n",
    "\n",
    "Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals\n",
    "\n",
    "* \\w - match word\n",
    "* \\d - match digit\n",
    "* \\s - match whitespace\n",
    "* \\W - match not word\n",
    "* \\D - match not digit\n",
    "* \\S - match not whitespace\n",
    "* [abc] - match any of a, b, or c\n",
    "* [^abc] - not match a, b, or c\n",
    "* [a-g] - match a character between a & g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8a152d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee77545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[^\\w]\"\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d4d12",
   "metadata": {},
   "source": [
    "# 6.Bag-of-words\n",
    "ML algorithm cannot word with raw text directly, we need to convert the text into vectors of numbers. this is called Feature Extraction. \n",
    "\n",
    "Bag-of-Words counts the number of times each word or n-gram (combination of n words) appears in a document\n",
    "\n",
    "Any information about the **Order** or **Structure** of a words is **discarded** that is why its called Bag of words. \n",
    "\n",
    "## Steps to create Bag-of-words\n",
    "\n",
    "### 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3db96b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Expression = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "126375eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome!', 'I like it.', 'Nice one.', 'I love it.']\n"
     ]
    }
   ],
   "source": [
    "sentence = nltk.sent_tokenize(Expression)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124b25e",
   "metadata": {},
   "source": [
    "### 2. Design the Vocabulary\n",
    "Let’s get all the unique words from the four loaded sentences ignoring the case, punctuation, and one-character tokens. These words will be our **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "547215a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77dedb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27127cc2",
   "metadata": {},
   "source": [
    "### 3. Create Document Vector\n",
    "Next, we need to score the words in each document. The task here is to convert each raw text into a vector of numbers. After that, we can use these vectors as input for a machine learning model.\n",
    "\n",
    " The simplest scoring method is to mark the presence of words with 1 for present and 0 for absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4e52aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7358df9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   0     0     0      0     0    0     1    1\n",
       "3        0      0     0   1     1     0      0     0    0     0    0\n",
       "4        0      0     0   0     0     0      0     1    1     0    0\n",
       "5        0      0     0   1     0     1      0     0    0     0    0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out(bag_of_words)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eff68970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome!', 'I like it.', 'Nice one.', 'I love it.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdba0f",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "The complexity of the bag_of_words model comes in deciding how to design the vocabulary of known works(tokens).\n",
    "\n",
    "#### Designing the Vocabulary\n",
    "When the vocabulary size increases, the vector representation of the documents also increases.\n",
    "\n",
    "Wile having **huge amount of data** the vector representation will have a **lot of zeros**. these vector which have a lot of zeros are called **sparse vector**. they require more memory and computational resources.\n",
    "\n",
    "Another more complex way to create a vocabulary is to use grouped words. This changes the scope of the vocabulary and allows the bag-of-words model to get **more details** about the document. This approach is called **n-grams**.\n",
    "\n",
    "An **n-gram** is a **sequence of a number of items** (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. The “n” in the “n-gram” refers to the number of the grouped words.\n",
    "\n",
    "Eg: Let's look at the bigrams for the following sentence:\n",
    "\"the Office building is open today\"\n",
    "\n",
    "All the bigrams are:\n",
    "\n",
    "* the office\n",
    "* office building\n",
    "* building is\n",
    "* is open\n",
    "* open today\n",
    "\n",
    "#### Scoring Words\n",
    "Once, we have created our vocabulary of known words, we need to score the occurrence of the words in our data. We saw one very simple approach - the binary approach (1 for presence, 0 for absence).\n",
    "\n",
    "Some additional scoring methods are:\n",
    "\n",
    "* **Counts**. Count the number of times each word appears in a document.\n",
    "* **Term-Frequencies**. Calculate the frequency that each word appears in document out of all the words in the document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc2e8a51",
   "metadata": {},
   "source": [
    "# 7. TF-IDF\n",
    "TF-IDF, short for **term frequency-inverse document frequency** is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
    "\n",
    "* Term Frequency (TF): a scoring of the frequency of the word in the current document.\n",
    "\n",
    "* Inverse Term Frequency (ITF): a scoring of how rare the word is across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0055d3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380907</td>\n",
       "      <td>0.451168</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.451168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380907</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.559022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.635091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439681</td>\n",
       "      <td>0.635091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645102</td>\n",
       "      <td>0.764096</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.82219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like     love     movie  \\\n",
       "0  0.000000  0.550195  0.000000  0.380907  0.451168  0.00000  0.451168   \n",
       "1  0.000000  0.000000  0.681722  0.000000  0.000000  0.00000  0.559022   \n",
       "2  0.635091  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.645102  0.764096  0.00000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.569213  0.000000  0.82219  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.380907  0.000000  \n",
       "1  0.000000  0.000000  0.471964  0.000000  \n",
       "2  0.000000  0.000000  0.439681  0.635091  \n",
       "3  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.707107  0.707107  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(sentence)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f12aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
